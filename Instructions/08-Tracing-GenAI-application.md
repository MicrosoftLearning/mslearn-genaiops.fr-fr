---
lab:
  title: Analyser et dÃ©boguer votre application dâ€™IA gÃ©nÃ©rative avec le traÃ§age
  description: 'DÃ©couvrez comment dÃ©boguer votre application dâ€™IA gÃ©nÃ©rative Ã  lâ€™aide du traÃ§age de son flux de travail, de lâ€™entrÃ©e utilisateur Ã  la rÃ©ponse du modÃ¨le, en passant par le post-traitement.'
---

# Analyser et dÃ©boguer votre application dâ€™IA gÃ©nÃ©rative avec le traÃ§age

Cet exercice prend environ **30**Â minutes.

> **Note**Â : cet exercice suppose une certaine connaissance dâ€™Azure AI Foundry, câ€™est pourquoi certaines instructions sont intentionnellement moins dÃ©taillÃ©es pour encourager une exploration plus active et un apprentissage pratique.

## Introduction

Dans cet exercice, vous allez exÃ©cuter un assistant dâ€™IA gÃ©nÃ©rative Ã  plusieurs Ã©tapes qui recommande des randonnÃ©es et suggÃ¨re des Ã©quipements de plein air. Vous allez utiliser les fonctionnalitÃ©s de traÃ§age du Kit de dÃ©veloppement logiciel (SDK) Azure AI Inference pour analyser la faÃ§on dont votre application sâ€™exÃ©cute et identifier les points de dÃ©cision clÃ©s pris par le modÃ¨le et la logique environnante.

Vous allez interagir avec un modÃ¨le dÃ©ployÃ© pour simuler un parcours utilisateur rÃ©el, tracer chaque Ã©tape de lâ€™application (de lâ€™entrÃ©e utilisateur Ã  la rÃ©ponse du modÃ¨le et au post-traitement) et afficher les donnÃ©es de trace dans AzureÂ AIÂ Foundry. Cela vous aidera Ã  comprendre comment le traÃ§age amÃ©liore lâ€™observabilitÃ©, simplifie le dÃ©bogage et prend en charge lâ€™optimisation des performances des applications dâ€™IA gÃ©nÃ©ratives.

## Configurer lâ€™environnement

Pour effectuer les tÃ¢ches de cet exercice, vous avez besoin des Ã©lÃ©ments suivantsÂ :

- Un hub Azure AI Foundry
- Un projet Azure AI Foundry
- Un modÃ¨le dÃ©ployÃ© (comme GPT-4o)
- Une ressource Application Insights connectÃ©e

### CrÃ©er un projet et un hub AI Foundry

Pour configurer rapidement un hub et un projet, des instructions simples pour utiliser lâ€™interface utilisateur du portail Azure AI Foundry sont fournies ci-dessous.

1. Dans un navigateur web, ouvrez le [portail AzureÂ AIÂ Foundry](https://ai.azure.com) Ã  lâ€™adresse `https://ai.azure.com` et connectez-vous en utilisant vos informations dâ€™identification Azure.
1. Sur la page dâ€™accueil, sÃ©lectionnez **+CrÃ©er un projet**.
1. Dans lâ€™assistant **CrÃ©er un projet**, saisissez un nom valide et, si un hub existant est suggÃ©rÃ©, choisissez lâ€™option permettant dâ€™en crÃ©er un. Passez ensuite en revue les ressources Azure qui seront crÃ©Ã©es automatiquement pour prendre en charge votre hub et votre projet.
1. SÃ©lectionnez **Personnaliser** et spÃ©cifiez les paramÃ¨tres suivants pour votre hubÂ :
    - **Nom du hub**Â : *un nom valide pour votre hub*
    - **Abonnement**Â : *votre abonnement Azure*
    - **Groupe de ressources**Â : *crÃ©ez ou sÃ©lectionnez un groupe de ressources*
    - **Emplacement**Â : sÃ©lectionnez **Aidez-moi Ã  choisir**, puis sÃ©lectionnez **gpt-4o** dans la fenÃªtre de lâ€™assistant de lâ€™emplacement et utilisez la rÃ©gion recommandÃ©e\*.
    - **Connecter Azure AI Services ou Azure OpenAI**Â : *crÃ©er une nouvelle ressource AI Services*
    - **Connecter la Recherche Azure AI** : ignorer la connexion

    > \* Les ressources Azure OpenAI sont limitÃ©es par des quotas de modÃ¨les rÃ©gionaux. Si une limite de quota est atteinte plus tard dans lâ€™exercice, vous devrez peut-Ãªtre crÃ©er une autre ressource dans une autre rÃ©gion.

1. SÃ©lectionnez **Suivant** et passez en revue votre configuration. SÃ©lectionnez **CrÃ©er** et patientez jusquâ€™Ã  ce que lâ€™opÃ©ration se termine.

### DÃ©ployer un modÃ¨le

Pour gÃ©nÃ©rer des donnÃ©es que vous pouvez surveiller, vous devez dâ€™abord dÃ©ployer un modÃ¨le et interagir avec celui-ci. Dans les instructions, vous Ãªtes invitÃ© Ã  dÃ©ployer un modÃ¨le GPT-4o, mais **vous pouvez utiliser nâ€™importe quel modÃ¨le** Ã  partir de la collection Azure OpenAI Service disponible.

1. Dans le menu de gauche, dans la section **Mes ressources**, sÃ©lectionnez la page **ModÃ¨les + points de terminaison**.
1. Dans le menu **+Â DÃ©ployer un modÃ¨le**, sÃ©lectionnez **DÃ©ployer le modÃ¨le de base**.
1. SÃ©lectionnez le modÃ¨le **gpt-4o** dans la liste et dÃ©ployez-le avec les paramÃ¨tres suivants en sÃ©lectionnant **Personnaliser** dans les dÃ©tails du dÃ©ploiementÂ :
    - **Nom du dÃ©ploiement**Â : *nom valide pour votre modÃ¨le de dÃ©ploiement*
    - **Type de dÃ©ploiement** : Standard
    - **Mise Ã  jour automatique de la version**Â : activÃ©e
    - **Version du modÃ¨le**Â : *sÃ©lectionnez la version la plus rÃ©cente disponible.*
    - **Ressource IA connectÃ©e**Â : *sÃ©lectionnez votre connexion de ressources AzureÂ OpenAI*
    - **Limite de dÃ©bit en jetons par minute (en milliers)** : 5Â 000
    - **Filtre de contenu**Â : DefaultV2
    - **Enable dynamic quota**Â : dÃ©sactivÃ©

    > **Remarque**Â : La rÃ©duction du nombre de jetons par minute permet dâ€™Ã©viter une surutilisation du quota disponible dans lâ€™abonnement que vous utilisez. 5Â 000Â TPM devraient suffire pour les donnÃ©es utilisÃ©es dans cet exercice. Si votre quota disponible est infÃ©rieur Ã  cette valeur, vous pourrez tout de mÃªme terminer lâ€™exercice, mais vous pourriez rencontrer des erreurs en cas de dÃ©passement de la limite.

1. Attendez la fin du dÃ©ploiement.

### Se connecter Ã  Application Insights

Connectez ApplicationÂ Insights Ã  votre projet dans AzureÂ AIÂ Foundry pour commencer Ã  collecter des donnÃ©es Ã  des fins dâ€™analyse.

1. Utilisez le menu de gauche, puis sÃ©lectionnez la page **Suivi**.
1. **CrÃ©ez** une ressource Application Insights pour vous connecter Ã  votre application.
1. Entrez un nom de ressource Application Insights, puis sÃ©lectionnez **CrÃ©er**.

ApplicationÂ Insights est dÃ©sormais connectÃ© Ã  votre projet et les donnÃ©es commencent Ã  Ãªtre collectÃ©es pour lâ€™analyse.

## ExÃ©cuter une application dâ€™IA gÃ©nÃ©rative avec CloudÂ Shell

Vous allez vous connecter Ã  votre projet AzureÂ AIÂ Foundry Ã  partir dâ€™AzureÂ CloudÂ Shell et interagir par programmation avec un modÃ¨le dÃ©ployÃ© dans le cadre dâ€™une application dâ€™IA gÃ©nÃ©rative.

### Interagir avec un modÃ¨le dÃ©ployÃ©

Commencez par rÃ©cupÃ©rer les informations nÃ©cessaires pour Ãªtre authentifiÃ© afin dâ€™interagir avec votre modÃ¨le dÃ©ployÃ©. Ensuite, accÃ©dez Ã  AzureÂ CloudÂ Shell et mettez Ã  jour le code de votre application dâ€™IA gÃ©nÃ©rative.

1. Dans le portail AzureÂ AIÂ Foundry, affichez la page **Vue dâ€™ensemble** de votre projet.
1. Dans la zone **DÃ©tails du projet**, notez la **chaÃ®ne de connexion du projet**.
1. **Enregistrez** la chaÃ®ne dans un bloc-notes. Vous utiliserez cette chaÃ®ne de connexion pour vous connecter Ã  votre projet dans une application cliente.
1. Ouvrez un nouvel onglet de navigateur (en gardant le portail AzureÂ AIÂ Foundry ouvert dans lâ€™onglet existant).
1. Dans un nouvel onglet, accÃ©dez au [portail Azure](https://portal.azure.com) Ã  lâ€™adresse `https://portal.azure.com` et connectez-vous en utilisant vos informations dâ€™identification Azure.
1. Utilisez le bouton **[\>_]** Ã  droite de la barre de recherche, en haut de la page, pour crÃ©er un environnement CloudÂ Shell dans le portail Azure, puis sÃ©lectionnez un environnement ***PowerShell*** avec aucun stockage dans votre abonnement.
1. Dans la barre dâ€™outils CloudÂ Shell, dans le menu **ParamÃ¨tres**, sÃ©lectionnez **AccÃ©der Ã  la version classique**.

    **<font color="red">Assurez-vous dâ€™avoir basculÃ© vers la version classique de CloudÂ Shell avant de continuer.</font>**

1. Dans le volet CloudÂ Shell, entrez et exÃ©cutez les commandes suivantesÂ :

    ```
    rm -r mslearn-genaiops -f
    git clone https://github.com/microsoftlearning/mslearn-genaiops mslearn-genaiops
    ```

    Cette commande clone le rÃ©fÃ©rentiel GitHub contenant les fichiers de code pour cet exercice.

1. Une fois le rÃ©fÃ©rentiel clonÃ©, accÃ©dez au dossier contenant les fichiers de code de lâ€™applicationÂ :  

    ```
   cd mslearn-genaiops/Files/08
    ```

1. Dans le volet de ligne de commande CloudÂ Shell, saisissez la commande suivante pour installer les bibliothÃ¨ques dont vous avez besoinÂ :

    ```
   python -m venv labenv
   ./labenv/bin/Activate.ps1
   pip install python-dotenv azure-identity azure-ai-projects azure-ai-inference azure-monitor-opentelemetry
    ```

1. Saisissez la commande suivante pour ouvrir le fichier de configuration fourniÂ :

    ```
   code .env
    ```

    Le fichier sâ€™ouvre dans un Ã©diteur de code.

1. Dans le fichier de codeÂ :

    1. Remplacez lâ€™espace rÃ©servÃ© **your_project_connection_string** par la chaÃ®ne de connexion de votre projet (copiÃ©e Ã  partir de la page **Vue dâ€™ensemble** du projet dans le portail AzureÂ AIÂ Foundry).
    1. Remplacez lâ€™espace rÃ©servÃ© **your_model_deployment** par le nom que vous avez attribuÃ© Ã  votre modÃ¨le de dÃ©ploiement GPT-4o (par dÃ©faut `gpt-4o`).

1. *Une* fois que vous avez remplacÃ© les espaces rÃ©servÃ©s, dans lâ€™Ã©diteur de code, utilisez la commande **CTRL+S** ou **Faites un clic droit sur > Enregistrer** pour **enregistrer vos modifications**, puis utilisez la commande **CTRL+Q** ou **Faites un clic droit > Quitter** pour fermer lâ€™Ã©diteur de code tout en gardant la ligne de commande Cloud Shell ouverte.

### Mettre Ã  jour le code de votre application dâ€™IA gÃ©nÃ©rative

Maintenant que votre environnement est configurÃ© et que votre fichier .env est configurÃ©, il est temps de prÃ©parer votre script dâ€™assistant IA pour lâ€™exÃ©cution. Outre la connexion Ã  un projet IA et lâ€™activation dâ€™ApplicationÂ Insights, vous devezÂ :

- Interagir avec votre modÃ¨le dÃ©ployÃ©
- DÃ©finir la fonction pour spÃ©cifier votre invite
- DÃ©finir le flux principal qui appelle toutes les fonctions

Vous allez ajouter ces trois parties Ã  un script de dÃ©part.

1. ExÃ©cutez la commande suivante pour **ouvrir le script** fourniÂ :

    ```
   code start-prompt.py
    ```

    Vous verrez que plusieurs lignes clÃ©s ont Ã©tÃ© laissÃ©es vides ou marquÃ©es avec des commentaires # vides. Votre tÃ¢che consiste Ã  terminer le script en copiant et en collant les lignes appropriÃ©es ci-dessous dans les emplacements appropriÃ©s.

1. Dans le script, recherchez **# Function to call the model and handle tracing**.
1. Sous ce commentaire, collez le code suivantÂ :

    ```
   def call_model(system_prompt, user_prompt, span_name):
        with tracer.start_as_current_span(span_name) as span:
            span.set_attribute("session.id", SESSION_ID)
            span.set_attribute("prompt.user", user_prompt)
            start_time = time.time()
    
            response = chat_client.complete(
                model=model_name,
                messages=[SystemMessage(system_prompt), UserMessage(user_prompt)]
            )
    
            duration = time.time() - start_time
            output = response.choices[0].message.content
            span.set_attribute("response.time", duration)
            span.set_attribute("response.tokens", len(output.split()))
            return output
    ```

1. Dans le script, recherchez **# Function to recommend a hike based on user preferences**.
1. Sous ce commentaire, collez le code suivantÂ :

    ```
   def recommend_hike(preferences):
        with tracer.start_as_current_span("recommend_hike") as span:
            prompt = f"""
            Recommend a named hiking trail based on the following user preferences.
            Provide only the name of the trail and a one-sentence summary.
            Preferences: {preferences}
            """
            response = call_model(
                "You are an expert hiking trail recommender.",
                prompt,
                "recommend_model_call"
            )
            span.set_attribute("hike_recommendation", response.strip())
            return response.strip()
    ```

1. Dans le script, recherchez **# ---- Main Flow ----**.
1. Sous ce commentaire, collez le code suivantÂ :

    ```
   if __name__ == "__main__":
       with tracer.start_as_current_span("trail_guide_session") as session_span:
           session_span.set_attribute("session.id", SESSION_ID)
           print("\n--- Trail Guide AI Assistant ---")
           preferences = input("Tell me what kind of hike you're looking for (location, difficulty, scenery):\n> ")

           hike = recommend_hike(preferences)
           print(f"\nâœ… Recommended Hike: {hike}")

           # Run profile function


           # Run match product function


           print(f"\nğŸ” Trace ID available in Application Insights for session: {SESSION_ID}")
    ```

1. **Enregistrez les modifications** que vous avez apportÃ©es au script.
1. Dans le volet de ligne de commande CloudÂ Shell, sous lâ€™Ã©diteur de code, entrez la commande suivante pour **exÃ©cuter le script**Â :

    ```
   python start-prompt.py
    ```

1. Donnez une description du type de randonnÃ©e que vous recherchez, par exempleÂ :

    ```
   A one-day hike in the mountains
    ```

    Le modÃ¨le gÃ©nÃ¨rera une rÃ©ponse, qui sera capturÃ©e avec ApplicationÂ Insights. Vous pouvez visualiser les traces dans le **portail AzureÂ AIÂ Foundry**.

> **Note**Â : lâ€™affichage des donnÃ©es de surveillance dans AzureÂ Monitor peut prendre quelques minutes.

## Afficher les donnÃ©es de traces dans le portail AzureÂ AIÂ Foundry

AprÃ¨s avoir exÃ©cutÃ© le script, vous avez capturÃ© une trace de lâ€™exÃ©cution de votre application dâ€™IA. Vous allez maintenant lâ€™explorer Ã  lâ€™aide dâ€™ApplicationÂ Insights dans AzureÂ AIÂ Foundry.

> **NoteÂ :** plus tard, vous rÃ©exÃ©cuterez le code et afficherez Ã  nouveau les traces dans le portail AzureÂ AIÂ Foundry. CommenÃ§ons par regarder oÃ¹ trouver les traces pour les visualiser.

### AccÃ©der au portail AzureÂ AIÂ Foundry

1. **Gardez CloudÂ Shell ouvertÂ !** Vous y reviendrez pour mettre Ã  jour le code et le rÃ©exÃ©cuter.
1. AccÃ©dez Ã  lâ€™onglet de votre navigateur avec le **portail Azure AI Foundry** ouvert.
1. Dans le menu de gauche, sÃ©lectionnez **Suivi**.
1. *Si* aucune donnÃ©e nâ€™est affichÃ©e, **actualisez** la vue.
1. SÃ©lectionnez la trace **train_guide_session** pour ouvrir une nouvelle fenÃªtre qui affiche plus de dÃ©tails.

### VÃ©rifier votre trace

Cette vue affiche la trace dâ€™une session complÃ¨te de lâ€™assistant dâ€™IA Trail Guide.

- **Ã‰tendue de niveau supÃ©rieur**Â : trail_guide_session. Il sâ€™agit de lâ€™Ã©tendue parente. Elle reprÃ©sente lâ€™ensemble de lâ€™exÃ©cution de votre assistant du dÃ©but Ã  la fin.

- **Ã‰tendues enfants imbriquÃ©es**Â : chaque ligne mise en retrait reprÃ©sente une opÃ©ration imbriquÃ©e. Vous y trouverezÂ :

    - **recommend_hike** qui capture votre logique pour dÃ©cider dâ€™une randonnÃ©e.
    - **recommend_model_call** qui est lâ€™Ã©tendue crÃ©Ã©e par call_model() Ã  lâ€™intÃ©rieur de recommend_hike.
    - **chat gpt-4o** qui est automatiquement instrumentÃ© par le Kit de dÃ©veloppement logiciel (SDK) AzureÂ AI Inference pour afficher lâ€™interaction LLM rÃ©elle.

1. Vous pouvez cliquer sur nâ€™importe quelle Ã©tendue pour afficherÂ :

    1. Sa durÃ©e.
    1. Ses attributs tels que lâ€™invite utilisateur, les jetons utilisÃ©s, le temps de rÃ©ponse.
    1. Toutes les erreurs ou donnÃ©es personnalisÃ©es attachÃ©es Ã  **span.set_attribute(...)**.

## Ajouter dâ€™autres fonctions Ã  votre code

1. AccÃ©dez Ã  lâ€™onglet de votre navigateur avec le **portail Azure** ouvert.
1. ExÃ©cutez la commande suivante pour **ouvrir de nouveau le scriptÂ :**

    ```
   code start-prompt.py
    ```

1. Dans le script, recherchez **# Function to generate a trip profile for the recommended hike**.
1. Sous ce commentaire, collez le code suivantÂ :

    ```
   def generate_trip_profile(hike_name):
       with tracer.start_as_current_span("trip_profile_generation") as span:
           prompt = f"""
           Hike: {hike_name}
           Respond ONLY with a valid JSON object and nothing else.
           Do not include any intro text, commentary, or markdown formatting.
           Format: {{ "trailType": ..., "typicalWeather": ..., "recommendedGear": [ ... ] }}
           """
           response = call_model(
               "You are an AI assistant that returns structured hiking trip data in JSON format.",
               prompt,
               "trip_profile_model_call"
           )
           print("ğŸ” Raw model response:", response)
           try:
               profile = json.loads(response)
               span.set_attribute("profile.success", True)
               return profile
           except json.JSONDecodeError as e:
               print("âŒ JSON decode error:", e)
               span.set_attribute("profile.success", False)
               return {}
    ```

1. Dans le script, recherchez **# Function to match recommended gear with products in the catalog**.
1. Sous ce commentaire, collez le code suivantÂ :

    ```
   def match_products(recommended_gear):
       with tracer.start_as_current_span("product_matching") as span:
           matched = []
           for gear_item in recommended_gear:
               for product in mock_product_catalog:
                   if any(word in product.lower() for word in gear_item.lower().split()):
                       matched.append(product)
                       break
           span.set_attribute("matched.count", len(matched))
           return matched
    ```

1. Dans le script, recherchez **# Run profile function**.
1. Ci-dessous et **alignÃ© avec** ce commentaire, collez le code suivantÂ :

    ```
           profile = generate_trip_profile(hike)
           if not profile:
               print("Failed to generate trip profile. Please check Application Insights for trace.")
               exit(1)

           print(f"\nğŸ“‹ Trip Profile for {hike}:")
           print(json.dumps(profile, indent=2))
    ```

1. Dans le script, recherchez **# Run match product function**.
1. Ci-dessous et **alignÃ© avec** ce commentaire, collez le code suivantÂ :

    ```
           matched = match_products(profile.get("recommendedGear", []))
           print("\nğŸ›’ Recommended Products from Lakeshore Retail:")
           print("\n".join(matched))
    ```

1. **Enregistrez les modifications** que vous avez apportÃ©es au script.
1. Dans le volet de ligne de commande CloudÂ Shell, sous lâ€™Ã©diteur de code, entrez la commande suivante pour **exÃ©cuter le script**Â :

    ```
   python start-prompt.py
    ```

1. Donnez une description du type de randonnÃ©e que vous recherchez, par exempleÂ :

    ```
   I want to go for a multi-day adventure along the beach
    ```

<br>
<details>
<summary><b>Script de solution</b>Â : Si votre code ne fonctionne pas.</summary><br>
<p>Si vous inspectez la trace LLM pour la fonction generate_trip_profile, vous remarquerez que la rÃ©ponse de lâ€™assistant inclut des backticks et le mot json pour mettre en forme la sortie en tant que bloc de code.

Bien que cela soit utile pour lâ€™affichage, cela entraÃ®ne des problÃ¨mes dans le code, car la sortie nâ€™est plus du JSON valide. Cela entraÃ®ne une erreur dâ€™analyse lors du traitement ultÃ©rieur.

Lâ€™erreur est probablement due Ã  la faÃ§on dont le LLM doit adhÃ©rer Ã  un format spÃ©cifique pour sa sortie. Lâ€™inclusion des instructions dans lâ€™invite utilisateur paraÃ®t plus efficace que de les placer dans lâ€™invite du systÃ¨me.</p>
</details>


> **Note**Â : lâ€™affichage des donnÃ©es de surveillance dans AzureÂ Monitor peut prendre quelques minutes.

### Afficher les nouvelles traces dans le portail AzureÂ AIÂ Foundry

1. Revenez au portail AzureÂ AIÂ Foundry.
1. Une nouvelle trace portant le mÃªme nom **trail_guide_session** doit apparaÃ®tre. Actualisez la vue si nÃ©cessaire.
1. SÃ©lectionnez la nouvelle trace pour ouvrir la vue plus dÃ©taillÃ©e.
1. Passez en revue les nouvelles Ã©tendues enfants imbriquÃ©es, **trip_profile_generation** et **product_matching**.
1. SÃ©lectionnez **product_matching** et passez en revue les mÃ©tadonnÃ©es qui sâ€™affichent.

    Dans la fonction product_matching, vous avez inclus **span.set_attribute("matched.count", len(matched)).** En dÃ©finissant lâ€™attribut avec la paire clÃ©-valeur **matched.count** et la longueur de la variable mise en correspondance, vous avez ajoutÃ© ces informations Ã  la trace **product_matching**. Vous trouverez cette paire clÃ©-valeur sous les **attributs** dans les mÃ©tadonnÃ©es.

## (FACULTATIF) Tracer une erreur

Si vous avez du temps supplÃ©mentaire, vous pouvez examiner comment utiliser les traces en cas dâ€™erreur. Un script susceptible de dÃ©clencher une erreur vous est fourni. ExÃ©cutez-le et passez les traces en revue.

Il sâ€™agit dâ€™un exercice conÃ§u pour vous mettre au dÃ©fi, ce qui signifie que les instructions sont intentionnellement moins dÃ©taillÃ©es.

1. Dans CloudÂ Shell, ouvrez le script **error-prompt.py**. Ce script se trouve dans le mÃªme rÃ©pertoire que le script **start-prompt.py**. VÃ©rifiez son contenu.
1. ExÃ©cutez le script **error-prompt.py**. Fournissez une rÃ©ponse dans la ligne de commande lorsque vous y Ãªtes invitÃ©.
1. *Normalement*, le message de sortie devrait inclure **Failed to generate trip profile. Please check Application Insights for trace.**.
1. AccÃ©dez Ã  la trace de **trip_profile_generation** et examinez pourquoi une erreur sâ€™est produite.

<br>
<details>
<summary><b>Obtenez une rÃ©ponse</b>Â : pourquoi vous avez pu rencontrer une erreur...</summary><br>
<p>Si vous inspectez la trace LLM pour la fonction generate_trip_profile, vous remarquerez que la rÃ©ponse de lâ€™assistant inclut des backticks et le mot json pour mettre en forme la sortie en tant que bloc de code.

Bien que cela soit utile pour lâ€™affichage, cela entraÃ®ne des problÃ¨mes dans le code, car la sortie nâ€™est plus du JSON valide. Cela entraÃ®ne une erreur dâ€™analyse lors du traitement ultÃ©rieur.

Lâ€™erreur est probablement due Ã  la faÃ§on dont le LLM doit adhÃ©rer Ã  un format spÃ©cifique pour sa sortie. Lâ€™inclusion des instructions dans lâ€™invite utilisateur paraÃ®t plus efficace que de les placer dans lâ€™invite du systÃ¨me.</p>
</details>

## OÃ¹ trouver dâ€™autres labos

Vous pouvez explorer dâ€™autres labos et exercices dans le [portail dâ€™apprentissage dâ€™Azure AI Foundry](https://ai.azure.com) ou consulter la **section de labo** du cours pour obtenir dâ€™autres activitÃ©s.
